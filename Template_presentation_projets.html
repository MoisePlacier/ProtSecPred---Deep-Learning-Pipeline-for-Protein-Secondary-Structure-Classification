<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="BOULET Faustine">
<meta name="author" content="BEAUFILS Constance">
<meta name="author" content="PLACIER Moïse">
<meta name="dcterms.date" content="2025-11-21">
<meta name="description" content="Prédiction de la structure secondaire à partir de la séquence, en évaluant l’impact de différents descripteurs biologiques et des méthodes Random Forest, CNN et modèles de NLP spécialisés du type BERT.">

<title>Prédire la structure secondaire des protéines : du signal biologique aux approches d’apprentissage avancées</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="Template_presentation_projets_files/libs/clipboard/clipboard.min.js"></script>
<script src="Template_presentation_projets_files/libs/quarto-html/quarto.js"></script>
<script src="Template_presentation_projets_files/libs/quarto-html/popper.min.js"></script>
<script src="Template_presentation_projets_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Template_presentation_projets_files/libs/quarto-html/anchor.min.js"></script>
<link href="Template_presentation_projets_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Template_presentation_projets_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Template_presentation_projets_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Template_presentation_projets_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Template_presentation_projets_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Prédire la structure secondaire des protéines : du signal biologique aux approches d’apprentissage avancées</h1>
<p class="subtitle lead">heure - passage</p>
</div>

<div>
  <div class="description">
    Prédiction de la structure secondaire à partir de la séquence, en évaluant l’impact de différents descripteurs biologiques et des méthodes Random Forest, CNN et modèles de NLP spécialisés du type BERT.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">BOULET Faustine </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institut Agro Rennes
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">BEAUFILS Constance </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Institut Agro Rennes
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">PLACIER Moïse </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            ENSAT &amp; Institut Agro Rennes
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 21, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="the-project-team" class="level1">
<h1>The Project Team</h1>
<div class="grid">
<div class="g-col-6 g-col-md-3 text-center">
<p><img src="../img_projets/Faustine_boulet.jpeg" alt="BOULET Faustine" class="photo-profil-about"> <strong>BOULET Faustine</strong> Etudiante ingénieure Agronome Institut Agro Rennes<br>
<a href="https://github.com/faustineboulet">GitHub</a> • <a href="https://www.linkedin.com/in/faustine-boulet-066451246/">LinkedIn</a></p>
</div>
<div class="g-col-6 g-col-md-3 text-center">
<p><img src="../img_projets/Constance_Beaufils.jpeg" alt="BEAUFILS Constance" class="photo-profil-about"> <strong>BEAUFILS Constance</strong> Etudiante ingénieure Agronome Institut Agro Rennes<br>
<a href="https://github.com/constancebfls">GitHub</a> • <a href="https://www.linkedin.com/in/constance-beaufils-940672259/">LinkedIn</a></p>
</div>
<div class="g-col-6 g-col-md-3 text-center">
<p><img src="../img_projets/Moise_Placier.png" alt="Moïse Placier" class="photo-profil-about"> <strong>PLACIER Moïse</strong> Etudiant ingénieur Agronome Institut Agro Rennes &amp; ENSAT <a href="https://github.com/MoisePlacier">GitHub</a> • <a href="www.linkedin.com/in/moïse-placier-9639bb24b">LinkedIn</a></p>
</div>
</div>
</section>
<section id="intro" class="level1">
<h1>Intro</h1>
<p>This project focuses on predicting the secondary structure of proteins from their primary amino-acid sequence. In the scientific scope, we aim to map from the linear sequence of residues (primary structure) to a one-dimensional annotation of local folding states (secondary structure).</p>
<section id="definitions" class="level3">
<h3 class="anchored" data-anchor-id="definitions">Definitions</h3>
<p><strong>Primary structure</strong> refers to the linear sequence of amino acids (residues) in a protein.</p>
<p><img src="../img_projets/5.svg" class="img-fluid"></p>
<p><strong>The secondary structure</strong> describes the local organization of the protein backbone. Most residues adopt one of two regular conformations: α-helices and β-strands, as originally formalized in the early structural work of <a href="https://pubs.acs.org/doi/10.1021/ja01161a053">Pauling &amp; Corey</a>.</p>
<p><img src="../img_projets/Secondary_structure.png" class="img-fluid"></p>
<p>For modelling, we simplify the standard eight secondary structure categories into three classes:</p>
<p>“H” for helix (including 3-10 helix “G” and π-helix “I” mapped to ‘H’)</p>
<p>“E” for β-strand (including β-bridge “B”)</p>
<p>“C” for coil (turns “T”, bends “S”, loops “L”)</p>
<p>This allows us to represent the secondary structure as a one-dimensional sequence of H/E/C labels.</p>
<p><strong>Tertiary structure</strong> refers to the three-dimensional conformation of a protein under physiological conditions (temperature, pH, solvent etc.), as observed via X-ray crystallography, NMR or cryo-EM. It describes the spatial relationships among the secondary-structure elements.</p>
<p><img src="../img_projets/Domain_Homology.png" class="img-fluid"></p>
</section>
<section id="why-focus-on-sequence-based-prediction" class="level3">
<h3 class="anchored" data-anchor-id="why-focus-on-sequence-based-prediction">Why focus on sequence-based prediction?</h3>
<p>A central principle of molecular biophysics is that the amino-acid sequence largely determines the final three-dimensional fold, a concept supported both experimentally (e.g., <a href="https://en.wikipedia.org/wiki/Anfinsen%27s_dogma">Anfinsen’s dogma</a>) and theoretically. This makes structure prediction from sequence an attractive computational goal, especially because sequencing a protein is vastly simpler, faster, and cheaper (on the order of ~$100) than experimentally determining its structure via crystallography, NMR or cryo-EM (which often require years of work and involve very large costs). Consequently the field of protein-structure prediction is highly active.</p>
<p>However, the prediction problem is intrinsically difficult. The classical Levinthal paradox illustrates well the combinatorial explosion of possible conformations: if each residue can adopt approximately three stable states, a protein of 101 amino acids would have ~3^100 potential conformations. Exhaustive enumeration is therefore infeasible</p>
</section>
<section id="why-predict-secondary-structure" class="level3">
<h3 class="anchored" data-anchor-id="why-predict-secondary-structure">Why predict secondary structure?</h3>
<p>We restrict our focus to secondary-structure prediction because it simplifies both the modelling and the evaluation. Instead of predicting a 3D coordinate triplet (x, y, z) for every residue, we classify each residue into H, E or C. Metrics such as Q3 accuracy are straightforward to compute, and this one-dimensional output remains highly informative for understanding folding, guiding tertiary-structure prediction, and supporting downstream tasks in structural bioinformatics.</p>
<p>At the same time, although simplified, the biological input features (physico-chemical properties, multiple-sequence alignments, PSSM/evolutionary profiles) and the modelling strategies (Random Forest, CNNs, transformer/NLP-based embeddings) remain relevant and transferable to tertiary-structure prediction tasks. This project therefore offers a sound starting point for exploring the core themes of protein-structure prediction.</p>
</section>
<section id="dataset-proteinnet" class="level3">
<h3 class="anchored" data-anchor-id="dataset-proteinnet">Dataset: ProteinNet</h3>
<p>This project relies on <strong>ProteinNet</strong>, a curated dataset introduced by AlQuraishi and colleagues to standardize machine-learning benchmarks for protein structure prediction (<a href="https://arxiv.org/pdf/1902.00249">ProteinNet paper</a>). ProteinNet integrates sequence data, evolutionary information, and experimentally resolved structural annotations, and is designed to mirror the evaluation protocol of the CASP challenge.</p>
<section id="characteristics-of-protein-datasets" class="level4">
<h4 class="anchored" data-anchor-id="characteristics-of-protein-datasets">Characteristics of Protein Datasets</h4>
<p>Protein datasets differ fundamentally from standard machine-learning datasets, and these differences deeply impact training, evaluation, and generalization.</p>
<p><strong>Non-I.I.D. Nature and Evolutionary Coupling</strong></p>
<p>Proteins are not independent samples. They arise from evolutionary processes and thus share phylogenetic relationships. Many proteins within a dataset have detectable homology, violating the i.i.d. assumption that underpins most ML models. Two sequences may share a common ancestor even if they differ in function, which makes naïve dataset splitting misleading.</p>
<p><strong>High Structural and Sequential Redundancy</strong></p>
<p>Homologous proteins can exceed 90% sequence identity, meaning they are nearly identical at the residue level. This level of redundancy has no equivalent in common ML domains such as vision, where similar classes remain pixel-distinct. Without proper control of redundancy, models trivially memorize homologous examples rather than learning the underlying biophysics of folding.</p>
</section>
<section id="impact-on-machine-learning-training" class="level4">
<h4 class="anchored" data-anchor-id="impact-on-machine-learning-training">Impact on Machine-Learning Training</h4>
<p>These dataset characteristics create several pitfalls that must be explicitly mitigated.</p>
<p><strong>Severe Risk of Overfitting</strong></p>
<p>If training, validation, and test sets share proteins above 30–40% sequence identity, a model can achieve high accuracy simply by memorizing close homologs. This yields deceptively strong performance, especially on short-range structure, without genuine understanding of folding constraints. When homologous leakage occurs, a model does not generalize the folding process and will fail to predict structures for truly novel proteins, such as proteins from under-sampled species or newly sequenced metagenomic datasets. This undermines the biological utility of the predictor.</p>
</section>
<section id="implemented-solutions" class="level4">
<h4 class="anchored" data-anchor-id="implemented-solutions">Implemented Solutions</h4>
<p>ProteinNet employs several mechanisms to avoid these pitfalls.</p>
<p><strong>Homology-Aware Splitting Through Sequence Clustering for train and validation split</strong></p>
<p>Dataset splits are not random but based on sequence-identity clustering. Tools such as <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4489315/">jackHMMER</a> group proteins by similarity, ensuring that clusters assigned to training do not overlap with those assigned to validation. This preserves evolutionary independence across folds and yields more realistic performance estimates.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../img_projets/.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Homology-Aware Splitting</figcaption>
</figure>
</div>
<p><strong>Blind Evaluation via CASP Protocol</strong></p>
<p>ProteinNet uses the <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC6927249/">CASP (Critical Assessment of Structure Prediction)</a> evaluation framework, where models are tested on protein structures that were not publicly available at training time. This ensures transparent, unbiased assessment of generalization. CASP remains the gold standard benchmark for structure prediction.</p>
</section>
</section>
</section>
<section id="biological-inputs-and-modelling-strategy" class="level1">
<h1>Biological Inputs and Modelling Strategy</h1>
<section id="local-models-physico-chemical-features-random-forest" class="level2">
<h2 class="anchored" data-anchor-id="local-models-physico-chemical-features-random-forest">1. Local Models: Physico-Chemical Features + Random Forest</h2>
<p>Our initial modeling strategy leverages the intrinsic physico-chemical properties of individual amino acids and their immediate sequence context to predict secondary structure. By employing a sliding window approach, we capture the local interactions that drive helix and coil formation. This framework provides a straightforward yet biologically informed baseline</p>
<section id="data-preprocessing-for-the-random-forest" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-for-the-random-forest">Data Preprocessing for the Random Forest</h3>
<p>To prepare the input data for our Random Forest model, each protein sequence is first segmented into overlapping windows of size 11, such that for a sequence of length <span class="math inline">\(L\)</span>, <span class="math inline">\(L\)</span> windows are generated, each centered on a single residue. Within each window, individual amino acids are represented as vectors of physico-chemical descriptors derived from the <a href="https://www.genome.jp/aaindex/">AAindex1 database</a>. Residues that correspond to padding (‘-’) at sequence termini are assigned zero-filled vectors to maintain consistent dimensionality. The descriptors from all residues in a window are then concatenated to form a single flattened feature vector, which preserves local contextual information around each residue while producing a tabular format compatible with classical machine learning models.</p>
<p>We selected descriptors that influence secondary structure formation:</p>
<ol type="1">
<li><strong>Hydrophobicity / Hydrophilicity</strong></li>
</ol>
<p>Determines helix folding and internal beta-strand stability.</p>
<ul>
<li>ARGP820101 Hydrophobicity index (<a href="https://doi.org/10.1016/0022-2836(82)90193-9">Argos et al., 1982</a>)</li>
</ul>
<ol start="2" type="1">
<li><strong>Side-chain Volume / Size</strong></li>
</ol>
<p>Steric effects can favor loops versus compact helices.</p>
<ul>
<li>BIGC670101 Residue volume (<a href="ttps://doi.org/10.1016/0022-2836(67)90162-6">Bigelow, 1967</a>)</li>
<li>FAUJ880106 Max width of side chain (<a href="https://pubmed.ncbi.nlm.nih.gov/3209351/">Fauchere et al., 1988</a>)</li>
</ul>
<ol start="3" type="1">
<li><strong>Polarity</strong></li>
</ol>
<p>Influences hydrogen bonding and solvent exposure.</p>
<ul>
<li>CHAM820101 Polarizability (<a href="https://www.sciencedirect.com/science/article/abs/pii/0022519382901916">Charton-Charton, 1982</a>)</li>
<li>GRAR740102 Polarity (<a href="https://pubmed.ncbi.nlm.nih.gov/4843792/">Grantham, 1974</a>)</li>
<li>RADA880108 Mean polarity (<a href="https://pubmed.ncbi.nlm.nih.gov/3166998/">Radzicka-Wolfenden, 1988</a>)</li>
</ul>
<ol start="4" type="1">
<li><strong>Charge</strong></li>
</ol>
<p>Local electrostatics affect helix and beta-strand formation.</p>
<ul>
<li>FAUJ880111 Positive charge (<a href="https://pubmed.ncbi.nlm.nih.gov/3209351/">Fauchere et al., 1988</a>)</li>
<li>FAUJ880112 Negative charge (<a href="https://pubmed.ncbi.nlm.nih.gov/3209351/">Fauchere et al., 1988</a>)</li>
</ul>
<ol start="5" type="1">
<li><strong>Flexibility / Rigidity</strong></li>
</ol>
<p>Glycine is flexible, proline rigid; this affects loop/corner formation.</p>
<ul>
<li>BHAR880101 Flexibility index (<a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1399-3011.1988.tb01258.x">Bhaskaran-Ponnuswamy, 1988</a>)</li>
</ul>
<ol start="6" type="1">
<li><strong>Hydrogen-bond potential</strong></li>
</ol>
<p>Ability to donate/accept H-bonds stabilizes helices and strands.</p>
<ul>
<li>CHAM830107 Charge transfer parameter (Charton-Charton, 1983)</li>
<li>FAUJ880109 Number of hydrogen-bond donors (<a href="https://pubmed.ncbi.nlm.nih.gov/3209351/">Fauchere et al., 1988</a>)</li>
</ul>
<p>Following this preprocessing pipeline, we obtain a comprehensive tabular dataset suitable for Random Forest training. For a sliding window of size 11, each residue is represented by 11 × 11 = <strong>121 features</strong>, reflecting the concatenation of 11 physico-chemical descriptors across the local sequence context. The resulting training set comprises <strong>2,270,581 windows</strong>, with secondary structure labels distributed as <strong>1,011,153 coils (C)</strong>, <strong>472,863 beta strands (E)</strong>, and <strong>786,565 helices (H)</strong>.</p>
</section>
<section id="model-training" class="level3">
<h3 class="anchored" data-anchor-id="model-training">Model Training</h3>
<p>To train our Random Forest classifier efficiently, we first extracted a representative subset of <strong>200,000 windows</strong> from the training set, stratified by secondary structure labels to preserve class distributions (C: 1,011,153; E: 472,863; H: 786,565). This subset was used for rapid <strong>hyperparameter tuning</strong>, allowing systematic exploration of the number of estimators, tree depth, minimum samples per leaf, and the number of features considered at each split. Each combination of hyperparameters was evaluated on a held-out validation set using multiple metrics, including Q3 accuracy (the fraction of residues correctly classified into the three secondary structure classes H, E, and C), balanced Q3 accuracy and macro F1 score.</p>
<p>After exhaustive grid search, the optimal parameters were identified as <strong>200 trees</strong>, a <strong>maximum depth of 20</strong>, <strong>minimum samples per leaf of 1</strong>, and sqrt features considered at each split. With thoses hyperparameters determined, the final Random Forest was trained on the full training set and persisted to disk.</p>
</section>
<section id="results-and-discussion" class="level3">
<h3 class="anchored" data-anchor-id="results-and-discussion">Results and discussion</h3>
<p><strong>Overall Metrics</strong></p>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q3 Accuracy</td>
<td>0.665</td>
</tr>
<tr class="even">
<td>Balanced Accuracy</td>
<td>0.626</td>
</tr>
<tr class="odd">
<td>Macro F1</td>
<td>0.633</td>
</tr>
</tbody>
</table>
<p><strong>Class-wise Performance</strong></p>
<table class="table">
<thead>
<tr class="header">
<th>Class</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-score</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>H</td>
<td>0.63</td>
<td>0.68</td>
<td>0.65</td>
<td>3788</td>
</tr>
<tr class="even">
<td>E</td>
<td>0.65</td>
<td>0.43</td>
<td>0.52</td>
<td>2551</td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.69</td>
<td>0.77</td>
<td>0.73</td>
<td>5317</td>
</tr>
<tr class="even">
<td><strong>Macro Avg</strong></td>
<td>0.66</td>
<td>0.63</td>
<td>0.63</td>
<td>11656</td>
</tr>
<tr class="odd">
<td><strong>Weighted Avg</strong></td>
<td>0.66</td>
<td>0.66</td>
<td>0.66</td>
<td>11656</td>
</tr>
</tbody>
</table>
<p>Although Random Forests trained on <strong>local physico-chemical</strong> features provide a biologically interpretable baseline, they are inherently limited, which directly accounts for the observed performance. These models are fundamentally local, relying on sliding windows that capture only short-range interactions between residues.</p>
<p>Beta strands are fundamentally <strong>non-local</strong> structures, as their formation often involves interactions between residues that are distant in the primary sequence, unlike alpha helices, which are largely local and can be stabilized by as few as five consecutive residues through hydrogen bonding. Moreover, a given amino acid sequence can adopt a stable alpha helix in isolation but form a beta strand in the context of long-range interactions with other distant residues. This distinction explains why the model predicts helices and coils relatively well but struggles with beta strands. As Baldwin and Rose note, “the accuracy of secondary structure predictions is only 65–70%. This fact is usually interpreted to imply that the remaining variance of 30–35% is caused by non-local interactions” (<a href="https://www.sciencedirect.com/science/article/pii/S0968000498013462">Baldwin &amp; Rose, 2013</a>), which is consistent with the ~66% Q3 accuracy achieved by our Random Forest.</p>
<p><img src="../img_projets/Beta_Global_vs_alpha_local.webp" class="img-fluid"></p>
<p>Another limitation stems from the residue-level granularity of predictions, which can fragment continuous secondary structure elements. Helices and beta strands span multiple residues, but predicting each residue independently often produces biologically inconsistent patterns. For instance, a target sequence : [‘C’,<strong>‘H’,‘H’,‘H’,‘H’,‘H’,‘H’</strong>,‘C’,‘C’,‘C’,‘C’,‘E’,‘E’,‘E’,‘E’] may be predicted as : [‘C’,<strong>‘H’,‘H’,‘H’</strong>,‘C’,‘C’,‘E’,‘C’,‘C’,‘E’,‘E’,‘E’,‘E’,‘E’,‘E’] resulting in short, fragmented structures that do not reflect realistic motifs. This is particularly problematic for alpha helices, which require a minimum of five consecutive residues to form a stable helix. If only three consecutive residues are predicted as helix, it is unclear how to interpret them biologically: should they be converted to coil, extended to form a full helix, or treated as part of a beta strand? Such inconsistencies highlight the limitations of purely residue-level, local prediction approaches.</p>
<p>Finally, the Random Forest model itself has limitations: it treats features as independent and is <strong>invariant to permutations</strong>, so it cannot exploit sequential correlations or detect motifs across neighboring residues within the window as a CNN or transformer-based model could. Additionally, the input features—physico-chemical descriptors—represent only local properties and is a <strong>redundant information</strong>. Taken together, these factors naturally limit the model’s performance, making a plateau around 65–66% accuracy expected for purely local, feature-based methods.</p>
</section>
</section>
<section id="evolutionary-information-and-convolutional-models-1d-cnn-pssm" class="level2">
<h2 class="anchored" data-anchor-id="evolutionary-information-and-convolutional-models-1d-cnn-pssm">Evolutionary information and convolutional models (1D CNN + PSSM)</h2>
<p>This subsection introduces evolutionary descriptors, especially Position-Specific Scoring Matrices (PSSMs) derived from multiple sequence alignments, widely used in secondary-structure predictors.</p>
<section id="the-biological-input-pssm" class="level3">
<h3 class="anchored" data-anchor-id="the-biological-input-pssm">The biological input : PSSM</h3>
<p>A Position-Specific Scoring Matrix (PSSM) provides an evolutionary profile for each residue position within a protein sequence. For each position <span class="math inline">\(i\)</span> and amino acid <span class="math inline">\(a\)</span>, the PSSM encodes a substitution probability or score <span class="math inline">\(P(a \mid i)\)</span> derived from a Multiple Sequence Alignment (MSA). An MSA is a matrix-like arrangement of homologous sequences where each row represents a sequence and columns align residues considered evolutionarily equivalent.</p>
<p>Suppose we have the following MSA for a protein segment of length 5:</p>
<table class="table">
<thead>
<tr class="header">
<th>Sequence</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>S1</td>
<td>A</td>
<td>L</td>
<td>K</td>
<td>A</td>
<td>V</td>
</tr>
<tr class="even">
<td>S2</td>
<td>A</td>
<td>L</td>
<td>R</td>
<td>A</td>
<td>V</td>
</tr>
<tr class="odd">
<td>S3</td>
<td>A</td>
<td>I</td>
<td>K</td>
<td>A</td>
<td>I</td>
</tr>
<tr class="even">
<td>S4</td>
<td>A</td>
<td>L</td>
<td>K</td>
<td>A</td>
<td>V</td>
</tr>
</tbody>
</table>
<p>First, the multiple sequence alignment is scanned column by column to count how many times each amino acid occurs at position <span class="math inline">\(i\)</span>.</p>
<table class="table">
<thead>
<tr class="header">
<th>Position</th>
<th>A</th>
<th>L</th>
<th>I</th>
<th>K</th>
<th>R</th>
<th>V</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td>0</td>
<td>3</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td><strong>4</strong></td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td><strong>5</strong></td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Next, the counts are converted into positional probabilities <span class="math inline">\(P(a \mid i)\)</span> by dividing by the number of sequences in the MSA. For example, at position 2, <span class="math inline">\(P(L \mid 2)=3/4\)</span> and <span class="math inline">\(P(I \mid 2)=1/4\)</span>. These probabilities indicate how frequently each amino acid occurs at a given evolutionary position.</p>
<p>Finally, the positional probabilities are compared to the background frequency <span class="math inline">\(P(a)\)</span> of each amino acid in a large non-redundant protein database using the log-odds formula: <span class="math display">\[PSSM(a, i) = \log \frac{P(a \mid i)}{P(a)}\]</span></p>
<table class="table">
<colgroup>
<col style="width: 12%">
<col style="width: 36%">
<col style="width: 25%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>AA ↓ / Pos →</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td><strong>log(4/4 / 0.05)= log(20)= 3.00</strong></td>
<td>0</td>
<td>0</td>
<td><strong>3.00</strong></td>
<td>0</td>
</tr>
<tr class="even">
<td>L</td>
<td>0</td>
<td><strong>log(3/4 / 0.05)=2.30</strong></td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>I</td>
<td>0</td>
<td><strong>log(1/4 / 0.05)=1.61</strong></td>
<td>0</td>
<td>0</td>
<td><strong>1.61</strong></td>
</tr>
<tr class="even">
<td>K</td>
<td>0</td>
<td>0</td>
<td><strong>2.71</strong></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>R</td>
<td>0</td>
<td>0</td>
<td><strong>1.61</strong></td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>V</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td><strong>2.30</strong></td>
</tr>
</tbody>
</table>
<p>If <span class="math inline">\(P(a\mid i)\)</span> is higher than the background, the score is positive, indicating evolutionary enrichment of that amino acid at that position. If it is lower, the score is negative. In the example, position 1 is strongly enriched for A because <span class="math inline">\(P(A\mid1)=1.0\)</span> is much larger than the background probability <span class="math inline">\(P(A)=0.05\)</span>, giving a high log-odds value of about 3.00. In other words, a high positive PSSM score at position <span class="math inline">\(i\)</span> for amino acid <span class="math inline">\(a\)</span> indicates that <span class="math inline">\(a\)</span> occurs more often than expected by chance at that position among homologous sequences.</p>
<p>The PSSMs used in this project are derived from the ProteinNet dataset, where MSAs were generated using JackHMMER and weighted with <a href="(https://pubmed.ncbi.nlm.nih.gov/7966282/)">Henikoff position-based weights</a> to reduce the influence of closely related sequences.</p>
<p>For secondary structure prediction, PSSMs introduce a strong biological signal that is not present in raw amino acid sequences or physico-chemical descriptors. Evolutionarily conserved positions tend to correspond to structurally or functionally important residues, while positions tolerant to substitution often lie in loops or solvent-exposed regions.</p>
</section>
<section id="data-preprocessing-for-the-1d-cnn-model" class="level3">
<h3 class="anchored" data-anchor-id="data-preprocessing-for-the-1d-cnn-model">Data Preprocessing for the 1D CNN Model</h3>
<p>To leverage the sequential nature of proteins, the 1D convolutional neural network (CNN) operates on full-length sequences rather than fixed local windows. Unlike the Random Forest, which relies on sliding windows to capture short-range context, the CNN requires a consistent tensor shape across all proteins while preserving the residue order.</p>
<p>Each amino acid is represented as a 21-dimensional one-hot vector, encoding the 20 standard residues plus a dedicated padding token. Protein sequences vary in length, so both inputs and labels are padded to the maximum sequence length in the dataset. As a result, each protein is converted into an input tensor of shape <span class="math inline">\((L_{\text{max}}, 21)\)</span> and a label tensor of shape <span class="math inline">\((L_{\text{max}})\)</span>, with padded positions assigned a special index to be ignored during loss computation.</p>
<p>During training, input tensors are transposed to <span class="math inline">\((\text{Batch}, 21, L_{\text{max}})\)</span>, the format expected by PyTorch’s Conv1d layers.</p>
</section>
<section id="d-cnn-architecture" class="level3">
<h3 class="anchored" data-anchor-id="d-cnn-architecture">1D CNN Architecture</h3>
<p>The network consists of three consecutive 1D convolutional layers with kernel size 5 corresponding to the typical length of local structural motifs, progressively increasing the number of filters from 128 to 256 and then 512. Dropout layers (rate 0.5) follow the first two convolutions to mitigate overfitting. The final classification layer is a position-wise 1D convolution with kernel size one, producing logits over the three secondary structure classes (H, E, C) for each residue. Padding is applied to maintain the original sequence length throughout all convolutional layers.</p>
<p>This architecture was chosen to balance biological interpretability and computational efficiency: the kernel size reflects the scale of short-range interactions, the progressive increase in filter number allows hierarchical feature extraction, and the absence of pooling preserves positional information critical for residue-level classification. By combining sequence order and evolutionary profiles through PSSMs, the network can leverage both local residue context and evolutionary conservation to improve secondary structure prediction accuracy.</p>
<p>You then justify the use of 1D or 2D convolutional networks: CNNs apply learnable filters that slide along the sequence, sharing weights across positions and exploiting translational invariance (or equivariance). This yields a stronger ability to learn local sequence motifs and conserved patterns than Random Forests. However, CNNs remain inherently local. Although receptive fields can be widened, PSSMs themselves carry position-specific but non-contextual information, so long-range effects are still not represented explicitly.</p>
</section>
<section id="results-and-discussion-1" class="level3">
<h3 class="anchored" data-anchor-id="results-and-discussion-1">Results and discussion</h3>
<p><strong>Overall Metrics</strong></p>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Q3 Accuracy</td>
<td><strong>82.56%</strong></td>
</tr>
<tr class="even">
<td>Balanced Accuracy</td>
<td><strong>83%</strong></td>
</tr>
<tr class="odd">
<td>Macro F1</td>
<td><strong>83%%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Class-wise Performance</strong></p>
<table class="table">
<thead>
<tr class="header">
<th>Class</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-score</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>H</td>
<td>0.85</td>
<td>0.86</td>
<td>0.85</td>
<td>3788</td>
</tr>
<tr class="even">
<td>E</td>
<td>0.83</td>
<td>0.77</td>
<td>0.80</td>
<td>2551</td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.81</td>
<td>0.83</td>
<td>0.82</td>
<td>5317</td>
</tr>
<tr class="even">
<td><strong>Macro Avg</strong></td>
<td>0.83</td>
<td>0.82</td>
<td>0.82</td>
<td>11656</td>
</tr>
<tr class="odd">
<td><strong>Weighted Avg</strong></td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
<td>11656</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="language-model-proteinbert-for-secondary-structure-prediction" class="level2">
<h2 class="anchored" data-anchor-id="language-model-proteinbert-for-secondary-structure-prediction">language model ProteinBERT for Secondary Structure Prediction</h2>
<p>ProteinBERT is a deep language model specifically designed for protein sequences. Inspired by the Transformer/BERT architecture from natural language processing, it integrates both local (residue-level) and global (whole-sequence) representations, enabling efficient modeling of very long protein sequences. Despite its relatively modest size (~16 million parameters), ProteinBERT achieves state-of-the-art performance across multiple protein prediction benchmarks.</p>
<p>The model is pretrained using a dual-task self-supervised scheme on ~106 million non-redundant protein sequences from UniRef90. The pretraining tasks include masked language modeling, where random amino acids are corrupted and must be reconstructed, and Gene Ontology (GO) annotation prediction, where the model predicts functional annotations from partial input. This joint pretraining allows ProteinBERT to learn representations that capture both sequence patterns and functional context.</p>
<p>ProteinBERT embeddings encode a rich mixture of residue-level physicochemical properties, evolutionary information, and functional context. Pretraining on large, diverse protein sets allows the model to generalize to sequences lacking close homologs, while the GO prediction task provides global functional signals that improve local predictions.</p>
<p>While PSSMs encode only the likelihood of observing each amino acid at a given position, independently of other positions, ProteinBERT captures richer information that integrates residue-level context, long-range dependencies, and functional signals learned across millions of proteins. Consequently, whereas PSSMs are limited to evolutionary conservation and local substitution patterns, ProteinBERT embeddings incorporate both local sequence motifs and global structural and functional cues.This combination of local and global context makes ProteinBERT particularly suitable for predicting residue-level secondary structures.</p>
<section id="the-embedding-generation-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="the-embedding-generation-pipeline">The embedding-generation pipeline</h3>
<p>The embedding-generation pipeline follows a simple two-step procedure.</p>
<p>In the first step, each protein sequence is processed independently through the pretrained ProtBERT model. The sequence is tokenized into single-residue tokens, encoded with the ProtBERT tokenizer, and passed through the encoder in inference mode. Each amino-acid sequence of length <span class="math inline">\(L\)</span> is transformed by ProtBERT into a matrix of shape <span class="math inline">\((L, 1024)\)</span>, where each residue is represented by a 1024-dimensional contextual embedding. These embeddings are saved individually as .npy arrays, one per protein, and the associated secondary-structure labels are stored as vectors of length <span class="math inline">\(L\)</span> containing integer class indices.</p>
<p>During dataset loading, each sequence keeps its original shape <span class="math inline">\((L, 1024)\)</span> until batching. When a batch is formed, the variable-length sequences are padded along the residue axis, producing a tensor of size <span class="math inline">\((\text{Batch}, L_{\text{max}}, 1024)\)</span>, where <span class="math inline">\(L_{\text{max}}\)</span> is the longest sequence in the batch. Labels undergo the same padding procedure and form a tensor of size <span class="math inline">\((\text{Batch}, L_{\text{max}})\)</span>, with a dedicated padding index (−100) used to mask non-valid positions during loss computation.</p>
</section>
</section>
<section id="results-and-discussion-2" class="level2">
<h2 class="anchored" data-anchor-id="results-and-discussion-2">Results and discussion</h2>
<table class="table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.8223</td>
</tr>
<tr class="even">
<td>Balanced Accuracy</td>
<td>0.8149</td>
</tr>
<tr class="odd">
<td>Macro F1</td>
<td>0.8156</td>
</tr>
</tbody>
</table>
<table class="table">
<thead>
<tr class="header">
<th>Class</th>
<th>Precision</th>
<th>Recall</th>
<th>F1-score</th>
<th>Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>H</td>
<td>0.84</td>
<td>0.84</td>
<td>0.84</td>
<td>3788</td>
</tr>
<tr class="even">
<td>E</td>
<td>0.78</td>
<td>0.77</td>
<td>0.78</td>
<td>2551</td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
<td>5317</td>
</tr>
<tr class="even">
<td><strong>Macro Avg</strong></td>
<td>0.82</td>
<td>0.81</td>
<td>0.82</td>
<td>11656</td>
</tr>
<tr class="odd">
<td><strong>Weighted Avg</strong></td>
<td>0.82</td>
<td>0.82</td>
<td>0.82</td>
<td>11656</td>
</tr>
</tbody>
</table>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>